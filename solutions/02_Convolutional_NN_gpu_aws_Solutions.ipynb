{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h2><center><font color='black'>   Taking a Deep-Learning Dive with Keras </font></center></h2>\n",
    "\n",
    "<img src='../imgs/Deep_Dive_Keras_2.jpg' align='middle'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/cnn1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/cnn_family_a.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../imgs/cnn_family.png'/>\n",
    "<img src='../imgs/many_architectures.png'/>\n",
    "* bubble size is # of parameters\n",
    "* AlexNet  (62 million parameters) (final layer 6x 6 x 256 x 2096) ~ > 1/2 are coming from FC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/input.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../imgs/conv1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../imgs/conv_title.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../imgs/3D_Convolution_Animation.gif'/>\n",
    "\n",
    "**Definition of Convolution:**\n",
    "Integral that expresses overlap of one function as it is shifted over another\n",
    "\n",
    "**In terms of CNNs:**\n",
    "The input is one function, and the kernel is the other. We can implement the integral as \n",
    "a summation over a finite # of array elements.  The output is called the feature map (which we can think of as a weighted sum)\n",
    "\n",
    "<img src='../imgs/conv_a.png'/>\n",
    "<img src='../imgs/conv_bb.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../imgs/conv_layers.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/conv_filter.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='imgs/max_pool.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Pooling benefits: \n",
    "\n",
    "- Reduces parameters by 75% \n",
    "- Helps us to generalize our model: Provides an 'abstracted' form of our features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../imgs/relu_1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../imgs/relu2.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/fc.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='imgs/all_layers.png'/>\n",
    "\n",
    "Credits: [Yosinski](http://yosinski.com)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import keras.utils.np_utils\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D, ZeroPadding1D\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras. preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  And now,  CNN in Keras! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Macroarchitecture of VGG16\n",
    "<img src='../imgs/VGG.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../imgs/dress_two.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Working with CNNs &  [CIFAR dataset ](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes\n",
    "\n",
    "<img src ='../imgs/cifar.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_test=to_categorical(y_test)\n",
    "y_train=to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Let's work with VGG16 model again\n",
    "\n",
    "Lets import the model this time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, None, None, 64)1792        input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, None, None, 64)36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, None, None, 64)0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, None, None, 12873856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, None, None, 128147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, None, None, 1280           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, None, None, 256295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, None, None, 256590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, None, None, 256590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, None, None, 2560           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, None, None, 5121180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, None, None, 5122359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, None, None, 5122359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, None, None, 5120           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, None, None, 5122359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, None, None, 5122359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, None, None, 5122359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, None, None, 5120           block5_conv3[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 14714688\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "image_input (InputLayer)         (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_3 (Model)                  multiple              14714688    image_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 512)           0           model_3[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "fc1 (Dense)                      (None, 4096)          2101248     flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "fc2 (Dense)                      (None, 4096)          16781312    fc1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 10)            40970       fc2[0][0]                        \n",
      "====================================================================================================\n",
      "Total params: 33638218\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg16_conv = VGG16(weights=None, include_top=False)\n",
    "model_vgg16_conv.summary()\n",
    "\n",
    "# Tailor our input format (Note- Tensorflow input order!)\n",
    "input = Input(shape=(32,32,3),name = 'image_input')  # our input shape: (3,32,32)\n",
    "\n",
    "#Use the generated model \n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "\n",
    "#Add the fully-connected layers \n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#Create your own model \n",
    "my_model = Model(input=input, output=x)  # Using the Functional API\n",
    "\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "my_model.compile(optimizer=sgd, loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 2.3043 - acc: 0.1001Warning: could not reach RemoteMonitor root server at http://localhost:9000\n",
      "50000/50000 [==============================] - 235s - loss: 2.3043 - acc: 0.1000 - val_loss: 2.3043 - val_acc: 0.1000\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 193s - loss: 2.3043 - acc: 0.0986 - val_loss: 2.3040 - val_acc: 0.1000\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 193s - loss: 2.3044 - acc: 0.0997 - val_loss: 2.3033 - val_acc: 0.1000\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 193s - loss: 2.3045 - acc: 0.1000 - val_loss: 2.3035 - val_acc: 0.1000\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 193s - loss: 2.3043 - acc: 0.0984 - val_loss: 2.3043 - val_acc: 0.1000\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 193s - loss: 2.3044 - acc: 0.0987 - val_loss: 2.3044 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f041f165490>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_a=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "# lets look at our results while we are at it\n",
    "cb_b=keras.callbacks.RemoteMonitor(root='http://localhost:9000')\n",
    "\n",
    "\n",
    "my_model.fit(X_train, y_train,\n",
    "          batch_size=128, nb_epoch=10, verbose=1,\n",
    "          validation_data=(X_test, y_test),callbacks=[cb_a,cb_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our Training has flat-lined!  \n",
    "Let's try again: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-Along! \n",
    "\n",
    "Let's repeat the MNIST training by leveraging [Transfer Learning](http://cs231n.github.io/transfer-learning/)\n",
    "\n",
    "1) Reinstantiate your VGG16 model: this time set weights='imagenet'    \n",
    "2) Freeze all previous layers    \n",
    "3) We'll want to rescale our image features to be between 0 & 1:   \n",
    "\n",
    "hint: see Kera's [ImageDataGenerator Function](https://keras.io/preprocessing/image/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reload the data\n",
    "from keras.datasets import cifar10\n",
    "import cv2\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg16_conv = keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "#model_vgg16_conv.summary()\n",
    "\n",
    "#Create your own input format \n",
    "input = Input(shape=(32,32,3),name = 'image_input')  # our input shape: (3,32,32)\n",
    "\n",
    "#Use the generated model \n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "    \n",
    "\n",
    "#Add the fully-connected layers \n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#Create your own model \n",
    "my_model = Model(input=input, output=x)  # Using the Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freeze convolutional layers\n",
    "for layer in my_model.layers[:2]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for layer in my_model.layers:\n",
    "    print(layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# must use a very small learning rate \n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "my_model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 145s - loss: 1.3028 - acc: 0.5445 - val_loss: 1.2546 - val_acc: 0.5595\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.2838 - acc: 0.5516 - val_loss: 1.2123 - val_acc: 0.5782\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.2628 - acc: 0.5562 - val_loss: 1.2049 - val_acc: 0.5762\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.2443 - acc: 0.5649 - val_loss: 1.1921 - val_acc: 0.5790\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.2321 - acc: 0.5675 - val_loss: 1.1739 - val_acc: 0.5940\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.2204 - acc: 0.5748 - val_loss: 1.1753 - val_acc: 0.5905\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.2043 - acc: 0.5775 - val_loss: 1.1757 - val_acc: 0.5818\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.1933 - acc: 0.5842 - val_loss: 1.1322 - val_acc: 0.6044\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.1798 - acc: 0.5861 - val_loss: 1.1424 - val_acc: 0.5973\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 140s - loss: 1.1740 - acc: 0.5904 - val_loss: 1.1394 - val_acc: 0.6028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2dc386b50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_datagen.fit(X_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "validation_generator = test_datagen.flow(X_test, y_test, batch_size=32)\n",
    "\n",
    "# same callbacks!\n",
    "cb_a=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "# lets look at our results while we are at it\n",
    "#cb_b=keras.callbacks.RemoteMonitor(root='http://localhost:9000')\n",
    "\n",
    "# fine-tune the model\n",
    "my_model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch= X_train.shape[0],\n",
    "    nb_epoch=10,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=  X_test.shape[0],\n",
    "    callbacks=[cb_a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
